{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d7b44-c316-4cb2-969e-f59c644827e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, problems, answers, tokenizer, max_length=512):\n",
    "        self.input_texts = [p + \" \" + a for p, a in zip(problems, answers)]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.input_texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "def get_dataloader():\n",
    "    dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "    problems = dataset[\"problem\"][:100]  # Start with small sample\n",
    "    answers = dataset[\"answer\"][:100]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    full_dataset = TextDataset(problems, answers, tokenizer)\n",
    "    \n",
    "    return DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if os.name == 'nt' else 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Model setup\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scaler = GradScaler()\n",
    "    dataloader = get_dataloader()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        torch.save(model.state_dict(), f\"model_epoch{epoch}.pt\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Warning: Multiple GPUs detected but using single-GPU mode\")\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65448257-4f42-4070-93bb-490528d1c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Make sure you have a recent PyTorch version (2.0+) for \"from torch.amp import autocast, GradScaler\"\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, problems, answers, tokenizer, max_length=512):\n",
    "        # Combine problems and answers into single text samples\n",
    "        self.input_texts = [p + \" \" + a for p, a in zip(problems, answers)]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Explicitly enable truncation to avoid warnings\n",
    "        encoding = self.tokenizer(\n",
    "            self.input_texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "def get_dataloader():\n",
    "    # Load the dataset and take a small subset for demonstration\n",
    "    dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "    problems = dataset[\"problem\"][:100]\n",
    "    answers = dataset[\"answer\"][:100]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
    "    # If pad_token is missing, set it to eos_token to avoid warnings\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Create the dataset and DataLoader\n",
    "    full_dataset = TextDataset(problems, answers, tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if os.name == 'nt' else 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model and move it to the chosen device\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # Use GradScaler from torch.amp (for PyTorch 2.0+)\n",
    "    # If you must support older versions, remove the device_type argument or switch to torch.cuda.amp\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    dataloader = get_dataloader()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            # Explicitly specify device_type='cuda' for newer PyTorch versions\n",
    "            with autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} | Average Loss: {avg_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"model_epoch{epoch}.pt\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Warning: Multiple GPUs detected, but this script uses single-GPU mode.\")\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae2959-3844-4e86-acd7-09700dab019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler  # Corrected import\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "def single_gpu_training():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Running single GPU training on {device}...\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "    problems = dataset[\"problem\"]\n",
    "    answers = dataset[\"answer\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Dataset class\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, problems, answers, tokenizer, max_length=128):\n",
    "            self.texts = [p + \" \" + a for p, a in zip(problems, answers)]\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "    # DataLoader\n",
    "    dataset_obj = TextDataset(problems, answers, tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if os.name == 'nt' else 4\n",
    "    )\n",
    "\n",
    "    # Model setup\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            # Corrected autocast usage\n",
    "            with autocast():  # Now properly imported from torch.cuda.amp\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            if step % 10 == 0:\n",
    "                logging.info(f\"[Epoch {epoch} | Step {step}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        logging.info(f\"[Epoch {epoch}] Avg Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"model_single_gpu.pth\")\n",
    "    logging.info(\"Training completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.device_count() < 1:\n",
    "        logging.error(\"No CUDA devices available\")\n",
    "    else:\n",
    "        single_gpu_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08837961-bcb8-434b-a382-e9271c205bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import threading\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "import uvicorn\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "# Allow dynamic GPU memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "##############################\n",
    "#      LOCAL TRAINING CODE   #\n",
    "##############################\n",
    "\n",
    "def train(hyperparams):\n",
    "    \"\"\"\n",
    "    Performs local training using your provided single-GPU training code.\n",
    "    Hyperparameters (with defaults) are passed in as a dict.\n",
    "    Returns the model state as a bytes object.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"[Client] Running single GPU training on {device}...\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "    problems = dataset[\"problem\"]\n",
    "    answers = dataset[\"answer\"]\n",
    "\n",
    "    # Hyperparameters\n",
    "    lr = hyperparams.get(\"lr\", 2e-5)\n",
    "    epochs = hyperparams.get(\"epochs\", 2)\n",
    "    batch_size = hyperparams.get(\"batch_size\", 2)\n",
    "    max_length = hyperparams.get(\"max_length\", 128)\n",
    "\n",
    "    # Setup tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Define Dataset class\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, problems, answers, tokenizer, max_length):\n",
    "            self.texts = [p + \" \" + a for p, a in zip(problems, answers)]\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset_obj = TextDataset(problems, answers, tokenizer, max_length)\n",
    "    dataloader = DataLoader(\n",
    "        dataset_obj,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0 if os.name == 'nt' else 4\n",
    "    )\n",
    "\n",
    "    # Model setup\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            if step % 10 == 0:\n",
    "                logging.info(f\"[Epoch {epoch} | Step {step}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        logging.info(f\"[Epoch {epoch}] Avg Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Save the trained model state to an in-memory buffer and return bytes\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save(model.state_dict(), buffer)\n",
    "    buffer.seek(0)\n",
    "    logging.info(\"[Client] Training completed successfully\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "##############################\n",
    "#       AGGREGATION CODE     #\n",
    "##############################\n",
    "\n",
    "def aggregate_models(state_dicts):\n",
    "    \"\"\"\n",
    "    Simple federated averaging:\n",
    "      For each key in the state dict, averages the corresponding tensors.\n",
    "    \"\"\"\n",
    "    aggregated_state = {}\n",
    "    # Initialize with zeros using the first client's state\n",
    "    for key in state_dicts[0].keys():\n",
    "        aggregated_state[key] = torch.zeros_like(state_dicts[0][key])\n",
    "    # Sum up all client states\n",
    "    for state in state_dicts:\n",
    "        for key in state.keys():\n",
    "            aggregated_state[key] += state[key]\n",
    "    # Average\n",
    "    for key in aggregated_state:\n",
    "        aggregated_state[key] /= len(state_dicts)\n",
    "    return aggregated_state\n",
    "\n",
    "\n",
    "##############################\n",
    "#        FASTAPI SERVER      #\n",
    "##############################\n",
    "\n",
    "def run_server(port):\n",
    "    \"\"\"\n",
    "    The server (CPU-based) provides hyperparameters and accepts the aggregated model.\n",
    "    \"\"\"\n",
    "    app = FastAPI()\n",
    "    # Define hyperparameters; these may be updated dynamically\n",
    "    hyperparams = {\n",
    "        \"lr\": 2e-5,\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 2,\n",
    "        \"max_length\": 128\n",
    "    }\n",
    "\n",
    "    @app.get(\"/get_hyperparameters\")\n",
    "    async def get_hyperparameters():\n",
    "        return hyperparams\n",
    "\n",
    "    @app.post(\"/upload_model\")\n",
    "    async def upload_model(model: UploadFile = File(...)):\n",
    "        contents = await model.read()\n",
    "        try:\n",
    "            _ = torch.load(io.BytesIO(contents), map_location=\"cpu\")\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=f\"Failed to load model: {e}\")\n",
    "        filename = f\"integrated_model_{int(time.time())}.pth\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(contents)\n",
    "        logging.info(f\"[Server] Integrated model received and saved as {filename}\")\n",
    "        return {\"message\": \"Model received\", \"path\": filename}\n",
    "\n",
    "    logging.info(f\"[Server] Starting FastAPI server on port {port}...\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "\n",
    "##############################\n",
    "#      FASTAPI AGGREGATOR    #\n",
    "##############################\n",
    "\n",
    "# Globals for aggregator role\n",
    "client_models = []\n",
    "expected_clients_global = None\n",
    "server_addr_global = None\n",
    "\n",
    "def run_aggregator(port, server_addr, expected_clients):\n",
    "    \"\"\"\n",
    "    The aggregator (GPU-based) collects client model updates, aggregates them,\n",
    "    and sends the integrated model to the server.\n",
    "    \"\"\"\n",
    "    global expected_clients_global, server_addr_global\n",
    "    expected_clients_global = expected_clients\n",
    "    server_addr_global = server_addr\n",
    "\n",
    "    app = FastAPI()\n",
    "\n",
    "    @app.post(\"/upload_client_model\")\n",
    "    async def upload_client_model(model: UploadFile = File(...)):\n",
    "        global client_models\n",
    "        contents = await model.read()\n",
    "        try:\n",
    "            state_dict = torch.load(io.BytesIO(contents), map_location=\"cpu\")\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=f\"Failed to load client model: {e}\")\n",
    "        client_models.append(state_dict)\n",
    "        logging.info(f\"[Aggregator] Received client model ({len(client_models)}/{expected_clients_global})\")\n",
    "        if len(client_models) >= expected_clients_global:\n",
    "            threading.Thread(target=aggregate_and_send).start()\n",
    "        return {\"message\": \"Client model received\"}\n",
    "\n",
    "    def aggregate_and_send():\n",
    "        global client_models, server_addr_global\n",
    "        logging.info(\"[Aggregator] Aggregating client models...\")\n",
    "        aggregated_state = aggregate_models(client_models)\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(aggregated_state, buffer)\n",
    "        buffer.seek(0)\n",
    "        url = f\"{server_addr_global}/upload_model\"\n",
    "        files = {\"model\": (\"aggregated_model.pth\", buffer.getvalue())}\n",
    "        try:\n",
    "            response = requests.post(url, files=files)\n",
    "            logging.info(f\"[Aggregator] Aggregated model sent to server: {response.json()}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[Aggregator] Failed to send aggregated model: {e}\")\n",
    "        client_models = []\n",
    "\n",
    "    logging.info(f\"[Aggregator] Starting aggregator on port {port} expecting {expected_clients} clients.\\n\"\n",
    "                 f\"Aggregated model will be sent to server at {server_addr_global}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "\n",
    "##############################\n",
    "#         FASTAPI CLIENT     #\n",
    "##############################\n",
    "\n",
    "def run_client(aggregator_addr, server_addr):\n",
    "    \"\"\"\n",
    "    The client (GPU-based) fetches hyperparameters from the server, performs local training,\n",
    "    and uploads its trained model to the aggregator.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{server_addr}/get_hyperparameters\")\n",
    "        hyperparams = response.json()\n",
    "        logging.info(f\"[Client] Received hyperparameters: {hyperparams}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Client] Failed to retrieve hyperparameters: {e}\")\n",
    "        return\n",
    "\n",
    "    # Perform local training using the provided training function\n",
    "    model_bytes = train(hyperparams)\n",
    "\n",
    "    # Upload the model to the aggregator\n",
    "    buffer = io.BytesIO(model_bytes)\n",
    "    url = f\"{aggregator_addr}/upload_client_model\"\n",
    "    files = {\"model\": (\"client_model.pth\", buffer.getvalue())}\n",
    "    try:\n",
    "        resp = requests.post(url, files=files)\n",
    "        logging.info(f\"[Client] Uploaded model to aggregator: {resp.json()}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Client] Failed to upload model to aggregator: {e}\")\n",
    "\n",
    "\n",
    "##############################\n",
    "#            MAIN            #\n",
    "##############################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Federated Training with FastAPI Demo using single GPU training code\"\n",
    "    )\n",
    "    parser.add_argument(\"--role\", type=str, required=True,\n",
    "                        choices=[\"server\", \"aggregator\", \"client\"],\n",
    "                        help=\"Role to run: 'server', 'aggregator', or 'client'\")\n",
    "    parser.add_argument(\"--port\", type=int, default=5000,\n",
    "                        help=\"Port for the server or aggregator\")\n",
    "    parser.add_argument(\"--aggregator_addr\", type=str,\n",
    "                        help=\"For client role: aggregator's address (e.g., http://<aggregator_ip>:<port>)\")\n",
    "    parser.add_argument(\"--server_addr\", type=str,\n",
    "                        help=\"For client/aggregator role: server's address (e.g., http://<server_ip>:<port>)\")\n",
    "    parser.add_argument(\"--expected_clients\", type=int, default=2,\n",
    "                        help=\"(For aggregator) Expected number of client updates before aggregation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.role == \"server\":\n",
    "        run_server(args.port)\n",
    "    elif args.role == \"aggregator\":\n",
    "        if not args.server_addr:\n",
    "            logging.error(\"Aggregator role requires --server_addr\")\n",
    "            exit(1)\n",
    "        run_aggregator(args.port, args.server_addr, args.expected_clients)\n",
    "    elif args.role == \"client\":\n",
    "        if not args.aggregator_addr or not args.server_addr:\n",
    "            logging.error(\"Client role requires --aggregator_addr and --server_addr\")\n",
    "            exit(1)\n",
    "        run_client(args.aggregator_addr, args.server_addr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc765d-4687-4c81-b64f-aabe4851693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CUDA 11.x/12.x\n",
    "# !pip install torch transformers datasets bitsandbytes accelerate tqdm transformers datasets accelerate humanize fastapi uvicorn python-multipart\n",
    "# # For Windows users (specific bitsandbytes version):\n",
    "# !pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba33c9b8-5842-4eba-95ee-224037fe9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Check CUDA version\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is working\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
