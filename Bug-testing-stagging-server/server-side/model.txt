from fastapi import FastAPI, HTTPException
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

app = FastAPI()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_path = "integrated_model_1740061788.pth"
model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-125M")
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125M")
tokenizer.pad_token = tokenizer.eos_token

@app.get("/inference")
def inference(prompt: str):
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        output = model.generate(**inputs, max_length=50)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        return {"prompt": prompt, "response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
